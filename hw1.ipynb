{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced DL and RL: Домашнее задание 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первое ДЗ связано с обучением с подкреплением, и оно придумано для ситуации, когда нейронные сети ещё не нужны, и пространство состояний в целом достаточно маленькое, чтобы можно было обучить хорошую стратегию методами TD-обучения или другими методами обучения с подкреплением. Задание получилось, надеюсь, интересное, но в том числе и достаточно техническое, так что для решения придётся немножко попрограммировать. Поэтому в качестве решения ожидается ссылка на jupyter-ноутбук на вашем github (или публичный, или с доступом для snikolenko); ссылку обязательно нужно прислать в виде сданного домашнего задания на портале Академии. Любые комментарии, новые идеи и рассуждения на тему, как всегда, категорически приветствуются.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть первая, с блекджеком и стратегиями"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы будем обучаться играть в очень простую, но всё-таки знаменитую и популярную игру: блекджек. Правила блекджека достаточно просты; давайте начнём с самой базовой версии, которая реализована в OpenAI Gym:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* численные значения карт равны от 2 до 10 для карт от двойки до десятки, 10 для валетов, дам и королей;\n",
    "* туз считается за 11 очков, если общая сумма карт на руке при этом не превосходит 21 (по-английски в этом случае говорят, что на руке есть usable ace), и за 1 очко, если превосходит;\n",
    "* игроку раздаются две карты, дилеру — одна в открытую и одна в закрытую;\n",
    "* игрок может совершать одно из двух действий:\n",
    "-- hit  — взять ещё одну карту;\n",
    "-- stand — не брать больше карт;\n",
    "* если сумма очков у игрока на руках больше 21, он проигрывает (bust);\n",
    "* если игрок выбирает stand с суммой не больше 21, дилер добирает карты, пока сумма карт в его руке меньше 17;\n",
    "* после этого игрок выигрывает, если дилер либо превышает 21, либо получает сумму очков меньше, чем сумма очков у игрока; при равенстве очков объявляется ничья (ставка возвращается);\n",
    "* в исходных правилах есть ещё дополнительный бонус за natural blackjack: если игрок набирает 21 очко с раздачи, двумя картами, он выигрывает не +1, а +1.5 (полторы ставки).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Именно этот простейший вариант блекджека реализован в OpenAI Gym:\n",
    "https://github.com/openai/gym/blob/38a1f630dc9815a567aaf299ae5844c8f8b9a6fa/gym/envs/toy_text/blackjack.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Рассмотрим очень простую стратегию: говорить stand, если у нас на руках комбинация в 19, 20 или 21 очко, во всех остальных случаях говорить hit. Используйте методы Монте-Карло, чтобы оценить выигрыш от этой стратегии."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Реализуйте метод обучения с подкреплением без модели (можно Q-обучение, но рекомендую попробовать и другие, например Monte Carlo control) для обучения стратегии в блекджеке, используя окружение Blackjack-v0 из OpenAI Gym."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Сколько выигрывает казино у вашей стратегии? Нарисуйте графики среднего дохода вашего метода (усреднённого по крайней мере по 100000 раздач, а лучше больше) по ходу обучения. Попробуйте подобрать оптимальные гиперпараметры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Blackjack-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:13<00:00, 7609.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward = -0.11344051520638049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def simple_strategy(state):\n",
    "    return True if (state[0] > 18) else False\n",
    "\n",
    "rewards = []\n",
    "n = 100000\n",
    "\n",
    "for i in tqdm(range(n)):\n",
    "    current_state = env.reset()\n",
    "    done = 0\n",
    "    while not done:\n",
    "        if simple_strategy(current_state):\n",
    "            next_state, reward, done, info = env.step(0)\n",
    "        else:\n",
    "            next_state, reward, done, info = env.step(1)\n",
    "        current_state = next_state\n",
    "        rewards.append(reward)\n",
    "\n",
    "print(f\"Reward = {np.mean(rewards)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пользовался https://towardsdatascience.com/learning-to-win-blackjack-with-monte-carlo-methods-61c90a52d53e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probs(Q_s, epsilon, nA):\n",
    "    policy_s = np.ones(nA) * epsilon / nA\n",
    "    best_a = np.argmax(Q_s)\n",
    "    policy_s[best_a] = 1 - epsilon + (epsilon / nA)\n",
    "    return policy_s\n",
    "\n",
    "\n",
    "def update_Q(env, episode, Q, alpha, gamma):\n",
    "    for s, a, r in episode:\n",
    "        first_occurence_idx = next(i for i,x in enumerate(episode) if x[0] == s)\n",
    "        G = sum([x[2]*(gamma**i) for i,x in enumerate(episode[first_occurence_idx:])])\n",
    "        Q[s][a] = Q[s][a] + alpha*(G - Q[s][a])\n",
    "    \n",
    "    return Q\n",
    "\n",
    "def play_game(env, Q, epsilon, nA):\n",
    "    episode = []\n",
    "    state = env.reset()\n",
    "    done = 0\n",
    "    while not done:\n",
    "        probs = get_probs(Q[state], epsilon, nA)\n",
    "        action = np.random.choice(np.arange(nA), p=probs) \\\n",
    "                                    if state in Q else env.action_space.sample()\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_state\n",
    "\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_control(env, num_episodes, epsilon = 1.0, alpha = 0.001, gamma = 1.0, nA=2):\n",
    "    Q = defaultdict(lambda: np.zeros(nA))\n",
    "    \n",
    "    for i_episode in tqdm(range(0, num_episodes)):\n",
    "        episode = play_game(env, Q, epsilon, nA)\n",
    "        Q = update_Q(env, episode, Q, alpha, gamma)\n",
    "    \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:23<00:00, 4339.24it/s]\n",
      "  0%|          | 319/100000 [00:00<00:31, 3183.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon = 0.7\t Alpha =0.0001\n",
      "Reward = -0.0018257768969188646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:22<00:00, 4543.58it/s]\n",
      "  0%|          | 359/100000 [00:00<00:27, 3588.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon = 0.7\t Alpha =0.001\n",
      "Reward = -0.01097752302130196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:21<00:00, 4663.40it/s]\n",
      "  0%|          | 394/100000 [00:00<00:25, 3931.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon = 0.7\t Alpha =0.01\n",
      "Reward = -0.03812607213808521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:23<00:00, 4342.01it/s]\n",
      "  0%|          | 455/100000 [00:00<00:21, 4547.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon = 0.7\t Alpha =0.1\n",
      "Reward = -0.06503280414255827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:26<00:00, 3751.27it/s]\n",
      "  1%|          | 730/100000 [00:00<00:27, 3586.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon = 0.8\t Alpha =0.0001\n",
      "Reward = -0.0020148708674595124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:23<00:00, 4331.18it/s]\n",
      "  0%|          | 270/100000 [00:00<00:36, 2698.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon = 0.8\t Alpha =0.001\n",
      "Reward = -0.013171777268768382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:22<00:00, 4419.21it/s]\n",
      "  0%|          | 422/100000 [00:00<00:23, 4216.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon = 0.8\t Alpha =0.01\n",
      "Reward = -0.04011648116259256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:24<00:00, 4125.92it/s]\n",
      "  0%|          | 340/100000 [00:00<00:29, 3396.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon = 0.8\t Alpha =0.1\n",
      "Reward = -0.04316217547521262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:29<00:00, 3338.79it/s]\n",
      "  0%|          | 360/100000 [00:00<00:28, 3499.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon = 0.85\t Alpha =0.0001\n",
      "Reward = -0.0023732830064374746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:25<00:00, 3850.10it/s]\n",
      "  0%|          | 443/100000 [00:00<00:22, 4427.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon = 0.85\t Alpha =0.001\n",
      "Reward = -0.015370651854945397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:24<00:00, 4008.18it/s]\n",
      "  0%|          | 457/100000 [00:00<00:21, 4567.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon = 0.85\t Alpha =0.01\n",
      "Reward = -0.031203108419205688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:24<00:00, 4164.81it/s]\n",
      "  0%|          | 320/100000 [00:00<00:31, 3197.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon = 0.85\t Alpha =0.1\n",
      "Reward = -0.04051959412288699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:26<00:00, 3834.99it/s]\n",
      "  0%|          | 373/100000 [00:00<00:26, 3729.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon = 0.9\t Alpha =0.0001\n",
      "Reward = -0.0023187824731115453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:21<00:00, 4668.22it/s]\n",
      "  0%|          | 324/100000 [00:00<00:30, 3239.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon = 0.9\t Alpha =0.001\n",
      "Reward = -0.016706156120547072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:20<00:00, 4829.99it/s]\n",
      "  0%|          | 361/100000 [00:00<00:27, 3609.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon = 0.9\t Alpha =0.01\n",
      "Reward = -0.044879376479584995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:21<00:00, 4752.27it/s]\n",
      "  0%|          | 183/100000 [00:00<00:54, 1829.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon = 0.9\t Alpha =0.1\n",
      "Reward = -0.028234280042845096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:24<00:00, 4008.79it/s]\n",
      "  0%|          | 344/100000 [00:00<00:28, 3437.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon = 0.95\t Alpha =0.0001\n",
      "Reward = -0.002334805886648607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:24<00:00, 4070.30it/s]\n",
      "  0%|          | 357/100000 [00:00<00:27, 3568.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon = 0.95\t Alpha =0.001\n",
      "Reward = -0.015837634110983028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:20<00:00, 4884.61it/s]\n",
      "  0%|          | 344/100000 [00:00<00:29, 3435.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon = 0.95\t Alpha =0.01\n",
      "Reward = -0.04400344534301899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:20<00:00, 4893.76it/s]\n",
      "  1%|          | 811/100000 [00:00<00:23, 4190.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon = 0.95\t Alpha =0.1\n",
      "Reward = -0.05215027256083667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:21<00:00, 4734.31it/s]\n",
      "  0%|          | 329/100000 [00:00<00:30, 3289.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon = 1.0\t Alpha =0.0001\n",
      "Reward = -0.002390379332378679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:20<00:00, 4796.31it/s]\n",
      "  0%|          | 140/100000 [00:00<01:11, 1389.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon = 1.0\t Alpha =0.001\n",
      "Reward = -0.01657265193507249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:28<00:00, 3458.95it/s]\n",
      "  0%|          | 266/100000 [00:00<00:37, 2656.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon = 1.0\t Alpha =0.01\n",
      "Reward = -0.048771285156329065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:26<00:00, 3787.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon = 1.0\t Alpha =0.1\n",
      "Reward = -0.07538160439066714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "eps = [0.7, 0.8, 0.85, 0.9, 0.95, 1.0]\n",
    "alph = [0.0001, 0.001, 0.01, 0.1]\n",
    "for i in eps:\n",
    "    for j in alph:\n",
    "        Q = mc_control(env, 100000, epsilon=i, alpha=j)\n",
    "        print(f\"Epsilon = {i}\\t Alpha ={j}\\nReward = {np.mean(list(np.max(v) for k, v in Q.items()))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть вторая, удвоенная\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В базовый блекджек, описанный в предыдущем разделе, обыграть казино вряд ли получится. Но, к счастью, на этом история не заканчивается. Описанные выше правила были упрощёнными, а на самом деле у игрока есть ещё и другие возможности. Реализовывать split может оказаться непросто, поэтому давайте ограничимся удвоением ставки. Итак, у игрока появляется дополнительное действие:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* double — удвоить ставку; при этом больше действий делать нельзя, игроку выдаётся ровно одна дополнительная карта, а выигрыш или проигрыш удваивается.\n",
    "4. Реализуйте новый вариант блекджека на основе окружения Blackjack-v0 из OpenAI Gym, в котором разрешено удвоение ставки.\n",
    "5. Реализуйте метод обучения с подкреплением без модели для этого варианта, постройте графики, аналогичные п.2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "\n",
    "def cmp(a, b):\n",
    "    return float(a > b) - float(a < b)\n",
    "\n",
    "# 1 = Ace, 2-10 = Number cards, Jack/Queen/King = 10\n",
    "deck = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 10, 10]\n",
    "\n",
    "\n",
    "def draw_card(np_random):\n",
    "    return int(np_random.choice(deck))\n",
    "\n",
    "\n",
    "def draw_hand(np_random):\n",
    "    return [draw_card(np_random), draw_card(np_random)]\n",
    "\n",
    "\n",
    "def usable_ace(hand):  # Does this hand have a usable ace?\n",
    "    return 1 in hand and sum(hand) + 10 <= 21\n",
    "\n",
    "\n",
    "def sum_hand(hand):  # Return current hand total\n",
    "    if usable_ace(hand):\n",
    "        return sum(hand) + 10\n",
    "    return sum(hand)\n",
    "\n",
    "\n",
    "def is_bust(hand):  # Is this hand a bust?\n",
    "    return sum_hand(hand) > 21\n",
    "\n",
    "\n",
    "def score(hand):  # What is the score of this hand (0 if bust)\n",
    "    return 0 if is_bust(hand) else sum_hand(hand)\n",
    "\n",
    "\n",
    "def is_natural(hand):  # Is this hand a natural blackjack?\n",
    "    return sorted(hand) == [1, 10]\n",
    "\n",
    "\n",
    "class BlackjackDoubleEnv(gym.Env):\n",
    "    \"\"\"Simple blackjack environment\n",
    "    Blackjack is a card game where the goal is to obtain cards that sum to as\n",
    "    near as possible to 21 without going over.  They're playing against a fixed\n",
    "    dealer.\n",
    "    Face cards (Jack, Queen, King) have point value 10.\n",
    "    Aces can either count as 11 or 1, and it's called 'usable' at 11.\n",
    "    This game is placed with an infinite deck (or with replacement).\n",
    "    The game starts with dealer having one face up and one face down card, while\n",
    "    player having two face up cards. (Virtually for all Blackjack games today).\n",
    "    The player can request additional cards (hit=1) until they decide to stop\n",
    "    (stick=0) or exceed 21 (bust).\n",
    "    After the player sticks, the dealer reveals their facedown card, and draws\n",
    "    until their sum is 17 or greater.  If the dealer goes bust the player wins.\n",
    "    If neither player nor dealer busts, the outcome (win, lose, draw) is\n",
    "    decided by whose sum is closer to 21.  The reward for winning is +1,\n",
    "    drawing is 0, and losing is -1.\n",
    "    The observation of a 3-tuple of: the players current sum,\n",
    "    the dealer's one showing card (1-10 where 1 is ace),\n",
    "    and whether or not the player holds a usable ace (0 or 1).\n",
    "    This environment corresponds to the version of the blackjack problem\n",
    "    described in Example 5.1 in Reinforcement Learning: An Introduction\n",
    "    by Sutton and Barto.\n",
    "    http://incompleteideas.net/book/the-book-2nd.html\n",
    "    \"\"\"\n",
    "    def __init__(self, natural=False):\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        self.observation_space = spaces.Tuple((\n",
    "            spaces.Discrete(32),\n",
    "            spaces.Discrete(11),\n",
    "            spaces.Discrete(2)))\n",
    "        self.seed()\n",
    "\n",
    "        # Flag to payout 1.5 on a \"natural\" blackjack win, like casino rules\n",
    "        # Ref: http://www.bicyclecards.com/how-to-play/blackjack/\n",
    "        self.natural = natural\n",
    "        # Start the first game\n",
    "        self.reset()\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def step(self, action):\n",
    "        if action == 1:  # hit: add a card to players hand and return\n",
    "            self.player.append(draw_card(self.np_random))\n",
    "            if is_bust(self.player):\n",
    "                done = True\n",
    "                reward = -1.\n",
    "            else:\n",
    "                done = False\n",
    "                reward = 0.\n",
    "        elif action == 0:  # stick: play out the dealers hand, and score\n",
    "            done = True\n",
    "            while sum_hand(self.dealer) < 17:\n",
    "                self.dealer.append(draw_card(self.np_random))\n",
    "            reward = cmp(score(self.player), score(self.dealer))\n",
    "            if self.natural and is_natural(self.player) and reward == 1.:\n",
    "                reward = 1.5\n",
    "        elif action == 2:\n",
    "            self.player.append(draw_card(self.np_random))\n",
    "            done = True\n",
    "            while sum_hand(self.dealer) < 17:\n",
    "                self.dealer.append(draw_card(self.np_random))\n",
    "            reward = cmp(score(self.player), score(self.dealer))\n",
    "            if self.natural and is_natural(self.player) and reward == 1.:\n",
    "                reward = 1.5\n",
    "            reward *= 2\n",
    "        return self._get_obs(), reward, done, {}\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return (sum_hand(self.player), self.dealer[0], usable_ace(self.player))\n",
    "\n",
    "    def reset(self):\n",
    "        self.dealer = draw_hand(self.np_random)\n",
    "        self.player = draw_hand(self.np_random)\n",
    "        return self._get_obs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "double_env = BlackjackDoubleEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probs(Q_s, epsilon):\n",
    "    policy_s = np.ones(3) * epsilon / 3\n",
    "    best_a = np.argmax(Q_s)\n",
    "    policy_s[best_a] = 1 - epsilon + (epsilon / 3)\n",
    "    return policy_s\n",
    "\n",
    "\n",
    "def update_Q(env, episode, Q, alpha, gamma):\n",
    "    for s, a, r in episode:\n",
    "        first_occurence_idx = next(i for i,x in enumerate(episode) if x[0] == s)\n",
    "        G = sum([x[2]*(gamma**i) for i,x in enumerate(episode[first_occurence_idx:])])\n",
    "        Q[s][a] = Q[s][a] + alpha*(G - Q[s][a])\n",
    "    \n",
    "    return Q\n",
    "\n",
    "def play_game(env, Q, epsilon):\n",
    "    episode = []\n",
    "    state = env.reset()\n",
    "    done = 0\n",
    "    while not done:\n",
    "        probs = get_probs(Q[state], epsilon)\n",
    "        action = np.random.choice(np.arange(3), p=probs) \\\n",
    "                                    if state in Q else env.action_space.sample()\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_state\n",
    "\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_control(env, num_episodes, epsilon = 1.0, alpha = 0.001, gamma = 1.0):\n",
    "    Q = defaultdict(lambda: np.zeros(3))\n",
    "    \n",
    "    for i_episode in tqdm(range(0, num_episodes)):\n",
    "        episode = play_game(env, Q, epsilon)\n",
    "        Q = update_Q(env, episode, Q, alpha, gamma)\n",
    "    \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:20<00:00, 4813.77it/s]\n",
      "  0%|          | 350/100000 [00:00<00:28, 3496.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon = 0.7\t Alpha =0.0001\n",
      "Reward = -0.00021582405037602286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:19<00:00, 5055.97it/s]\n",
      "  0%|          | 349/100000 [00:00<00:28, 3479.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon = 0.7\t Alpha =0.001\n",
      "Reward = -0.001544441806075449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:20<00:00, 4991.52it/s]\n",
      "  0%|          | 496/100000 [00:00<00:20, 4941.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon = 0.7\t Alpha =0.01\n",
      "Reward = 0.02543871640514988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:20<00:00, 4992.67it/s]\n",
      "  1%|          | 813/100000 [00:00<00:26, 3745.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon = 0.7\t Alpha =0.1\n",
      "Reward = 0.052975830483178096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:20<00:00, 4934.45it/s]\n",
      "  0%|          | 388/100000 [00:00<00:25, 3869.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon = 0.8\t Alpha =0.0001\n",
      "Reward = -0.0005373318950394633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:24<00:00, 4139.98it/s]\n",
      "  0%|          | 408/100000 [00:00<00:24, 4078.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon = 0.8\t Alpha =0.001\n",
      "Reward = -0.001555726977560458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:27<00:00, 3577.33it/s]\n",
      "  1%|          | 581/100000 [00:00<00:38, 2597.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon = 0.8\t Alpha =0.01\n",
      "Reward = 0.019207189684852854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:25<00:00, 3955.14it/s]\n",
      "  0%|          | 385/100000 [00:00<00:25, 3846.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon = 0.8\t Alpha =0.1\n",
      "Reward = 0.0701452376230715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:21<00:00, 4600.81it/s]\n",
      "  1%|          | 746/100000 [00:00<00:25, 3828.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon = 0.85\t Alpha =0.0001\n",
      "Reward = -0.0007963688659419395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:20<00:00, 4945.80it/s]\n",
      "  0%|          | 386/100000 [00:00<00:25, 3856.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon = 0.85\t Alpha =0.001\n",
      "Reward = -0.0028595616748872976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:19<00:00, 5127.26it/s]\n",
      "  1%|          | 881/100000 [00:00<00:25, 3838.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon = 0.85\t Alpha =0.01\n",
      "Reward = 0.015429982930300202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:19<00:00, 5019.36it/s]\n",
      "  1%|          | 925/100000 [00:00<00:22, 4431.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon = 0.85\t Alpha =0.1\n",
      "Reward = 0.10237091930565612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:20<00:00, 4900.33it/s]\n",
      "  1%|          | 885/100000 [00:00<00:22, 4335.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon = 0.9\t Alpha =0.0001\n",
      "Reward = -0.0008206738380557489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:19<00:00, 5070.95it/s]\n",
      "  0%|          | 367/100000 [00:00<00:27, 3664.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon = 0.9\t Alpha =0.001\n",
      "Reward = -0.004056941002541495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:20<00:00, 4979.22it/s]\n",
      "  0%|          | 403/100000 [00:00<00:24, 4028.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon = 0.9\t Alpha =0.01\n",
      "Reward = 0.01203994948647839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:21<00:00, 4571.83it/s]\n",
      "  0%|          | 302/100000 [00:00<00:33, 3018.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon = 0.9\t Alpha =0.1\n",
      "Reward = 0.09909442022280256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:22<00:00, 4370.94it/s]\n",
      "  0%|          | 366/100000 [00:00<00:27, 3658.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon = 0.95\t Alpha =0.0001\n",
      "Reward = -0.000609794877598562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:20<00:00, 4847.08it/s]\n",
      "  0%|          | 293/100000 [00:00<00:34, 2929.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon = 0.95\t Alpha =0.001\n",
      "Reward = -0.004341756944017241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:21<00:00, 4740.02it/s]\n",
      "  0%|          | 366/100000 [00:00<00:27, 3653.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon = 0.95\t Alpha =0.01\n",
      "Reward = 0.011515571556055966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:20<00:00, 4774.16it/s]\n",
      "  0%|          | 403/100000 [00:00<00:24, 4025.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon = 0.95\t Alpha =0.1\n",
      "Reward = 0.07247711461626052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:20<00:00, 4977.55it/s]\n",
      "  0%|          | 462/100000 [00:00<00:21, 4618.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon = 1.0\t Alpha =0.0001\n",
      "Reward = -0.0006825074795277032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:20<00:00, 4940.98it/s]\n",
      "  0%|          | 386/100000 [00:00<00:25, 3846.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon = 1.0\t Alpha =0.001\n",
      "Reward = -0.005369861550728268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:20<00:00, 4939.48it/s]\n",
      "  0%|          | 456/100000 [00:00<00:21, 4559.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon = 1.0\t Alpha =0.01\n",
      "Reward = 0.005518438091750532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:19<00:00, 5087.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon = 1.0\t Alpha =0.1\n",
      "Reward = 0.0844147848265624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "eps = [0.7, 0.8, 0.85, 0.9, 0.95, 1.0]\n",
    "alph = [0.0001, 0.001, 0.01, 0.1]\n",
    "for i in eps:\n",
    "    for j in alph:\n",
    "        Q = mc_control(double_env, 100000, epsilon=i, alpha=j)\n",
    "        print(f\"Epsilon = {i}\\t Alpha ={j}\\nReward = {np.mean(list(np.max(v) for k, v in Q.items()))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть третья, в главной роли — Дастин Хоффман\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь давайте вспомним, как играют в блекджек настоящие профессионалы. Дело в том, что в оффлайн-казино обычно не перемешивают колоду после каждой раздачи — это слишком замедляло бы игру. После раздачи карты просто раздаются дальше с верха колоды до тех пор, пока карт не останется слишком мало, и только тогда колода перемешивается; давайте для определённости считать, что наше казино будет перемешивать колоду, в которой осталось меньше 15 карт."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Действительно, если вы будете запоминать, какие карты уже вышли, у вас будет информация о том, какие карты ещё остались, а это позволяет лучше понять, когда нужно удваивать ставку или делать split, а когда лучше не стоит. В настоящем казино могут раздавать карты сразу из нескольких колод, и заслуга Rain Man’а была в том, что он смог считать карты в шести колодах одновременно. Но мы с вами вооружены компьютерами, так что подсчёт можно считать автоматическим.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Реализуйте вариант окружения Blackjack-v0 из предыдущей части (с удвоением), в котором игрок имеет возможность “считать карты” в колоде. Это можно сделать разными способами; возможно, вам поможет статья википедии о блекджеке (а возможно, и нет).\n",
    "7. Реализуйте метод обучения с подкреплением без модели для этого варианта, постройте графики, аналогичные п.2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так и не смог, не успел("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть четвёртая, опциональная"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ну и напоследок ещё парочка опциональных заданий за дополнительные баллы.\n",
    "\n",
    "8. Реализуйте поиск стратегии в блекджеке с известной моделью из первой части, решив уравнения Беллмана для V* или Q*. Для этого вам придётся сначала оценить параметры модели, т.е. вероятности переходов между состояниями.\n",
    "9. Реализуйте вариант из второй или третьей части, в котором есть ещё возможность делать split: в случае, когда игроку пришли две одинаковые карты, он может разбить руку на две, внести ещё одну ставку и продолжать играть две руки сразу (как будто за двоих игроков). Скорее всего, обыграть казино получится только в варианте с разрешённым split’ом и подсчётом карт; если получится, это будет отличное завершение проекта!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
